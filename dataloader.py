{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom collections import Counter, defaultdict\nimport spacy\nimport random\n\n# 请确保在运行前安装好 spaCy 模型：\n# !python -m spacy download de_core_news_sm\n# !python -m spacy download en_core_web_sm\n\nspacy_de = spacy.load(\"de_core_news_sm\")\nspacy_en = spacy.load(\"en_core_web_sm\")\n\n# Tokenization\n\ndef tokenize_de(text):\n    return [tok.text for tok in spacy_de.tokenizer(text)]\n\ndef tokenize_en(text):\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\n# Special tokens\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\nBLANK_WORD = '<blank>'\nUNK_WORD = '<unk>'\nMIN_FREQ = 2\nMAX_LEN = 100\n\n# Vocab class\nclass Vocab:\n    def __init__(self, counter, min_freq=1, specials=None):\n        specials = specials or []\n        self.itos = list(specials)\n        token_set = set(specials)\n        self.counter = counter \n        for token, freq in counter.items():\n            if freq >= min_freq and token not in token_set:\n                self.itos.append(token)\n        # 自动将不在词表中的 token 映射为 <unk>\n        self.stoi = defaultdict(lambda: self.stoi[UNK_WORD])\n        for i, token in enumerate(self.itos):\n            self.stoi[token] = i\n\n    def __getitem__(self, token):\n        return self.stoi[token]\n\n    def __len__(self):\n        return len(self.itos)\n\n    def freq(self, token):\n        return self.counter[token]\n\n# Dataset class\nclass TranslationDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\n# Preprocessing\n\ndef preprocess(example):\n    src_tokens = tokenize_de(example[\"de\"])\n    tgt_tokens = tokenize_en(example[\"en\"])\n    if len(src_tokens) <= MAX_LEN and len(tgt_tokens) <= MAX_LEN:\n        return {\n            \"src\": src_tokens,\n            \"trg\": [BOS_WORD] + tgt_tokens + [EOS_WORD]\n        }\n    return None\n\ndef filter_none(example):\n    return example is not None\n\n# Build vocab\n\ndef build_vocab(data, key):\n    counter = Counter()\n    for ex in data:\n        counter.update(ex[key])\n    return Vocab(counter, min_freq=MIN_FREQ, specials=[UNK_WORD, BLANK_WORD, BOS_WORD, EOS_WORD])\n\n# Encode\n\ndef encode(example, src_vocab, tgt_vocab):\n    example[\"src_ids\"] = [src_vocab[token] for token in example[\"src\"]]\n    example[\"trg_ids\"] = [tgt_vocab[token] for token in example[\"trg\"]]\n    return example\n\n# Batch Sampler\n\nclass SortedBatchSampler(Sampler):\n    def __init__(self, lengths, batch_size, pool_size=100, shuffle=True):\n        self.lengths = lengths\n        self.batch_size = batch_size\n        self.pool_size = pool_size\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        indices = list(range(len(self.lengths)))\n        if self.shuffle:\n            random.shuffle(indices)\n\n        pooled_batches = []\n        for i in range(0, len(indices), self.batch_size * self.pool_size):\n            pool = indices[i: i + self.batch_size * self.pool_size]\n            pool = sorted(pool, key=lambda x: self.lengths[x])\n            batches = [pool[j: j + self.batch_size] for j in range(0, len(pool), self.batch_size)]\n            if self.shuffle:\n                random.shuffle(batches)\n            pooled_batches.extend(batches)\n\n        return iter([idx for batch in pooled_batches for idx in batch])\n\n    def __len__(self):\n        return len(self.lengths)\n\n# Collate function\n\ndef collate_fn(batch, pad_idx):\n    src_batch = [example[\"src_ids\"] for example in batch]\n    trg_batch = [example[\"trg_ids\"] for example in batch]\n\n    def pad(sequences):\n        max_len = max(len(seq) for seq in sequences)\n        return [seq + [pad_idx] * (max_len - len(seq)) for seq in sequences]\n\n    src_padded = pad(src_batch)\n    trg_padded = pad(trg_batch)\n\n    return {\n        \"src\": torch.tensor(src_padded, dtype=torch.long),\n        \"trg\": torch.tensor(trg_padded, dtype=torch.long),\n    }\n\n# Load and prepare\n\ndef prepare_data(batch_size=64):\n    raw_data = load_dataset(\"iwslt2017\", \"iwslt2017-de-en\", split={\"train\": \"train\", \"validation\": \"validation\", \"test\": \"test\"})\n\n    train_data = list(filter(filter_none, map(preprocess, raw_data[\"train\"][\"translation\"])))\n    val_data = list(filter(filter_none, map(preprocess, raw_data[\"validation\"][\"translation\"])))\n    test_data = list(filter(filter_none, map(preprocess, raw_data[\"test\"][\"translation\"])))\n\n    src_vocab = build_vocab(train_data, \"src\")\n    tgt_vocab = build_vocab(train_data, \"trg\")\n\n    train_data = list(map(lambda ex: encode(ex, src_vocab, tgt_vocab), train_data))\n    val_data = list(map(lambda ex: encode(ex, src_vocab, tgt_vocab), val_data))\n    test_data = list(map(lambda ex: encode(ex, src_vocab, tgt_vocab), test_data))\n\n    pad_idx = src_vocab[BLANK_WORD]\n\n    train_dataset = TranslationDataset(train_data)\n    val_dataset = TranslationDataset(val_data)\n    test_dataset = TranslationDataset(test_data)\n\n    lengths = [len(example[\"src_ids\"]) for example in train_data]\n    sampler = SortedBatchSampler(lengths, batch_size=batch_size)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, collate_fn=lambda x: collate_fn(x, pad_idx))\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: collate_fn(x, pad_idx))\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: collate_fn(x, pad_idx))\n\n    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab","metadata":{"_uuid":"40069245-7c01-40b5-8b39-5577ccf4be79","_cell_guid":"8a9b8e1d-d87c-42d0-b30e-4cc402766de3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-22T12:16:45.461272Z","iopub.execute_input":"2025-04-22T12:16:45.461599Z","iopub.status.idle":"2025-04-22T12:16:58.626615Z","shell.execute_reply.started":"2025-04-22T12:16:45.461569Z","shell.execute_reply":"2025-04-22T12:16:58.625063Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1266058582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# !python -m spacy download en_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mspacy_de\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"de_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mspacy_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."],"ename":"OSError","evalue":"[E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.","output_type":"error"}],"execution_count":1}]}